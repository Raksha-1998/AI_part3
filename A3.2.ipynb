{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9pWsPAt0dCh"
   },
   "source": [
    "# Assignment 3 Part 2 - Find complex answers to medical questions\n",
    "\n",
    "*Submission deadline: Friday 24 May 2024, 11:55pm.*\n",
    "\n",
    "*Assessment marks: 15 marks (15% of the total unit assessment)*\n",
    "\n",
    "Unless a Special Consideration request has been submitted and approved, a 5% penalty (of the total possible mark of the task) will be applied for each day a written report or presentation assessment is not submitted, up until the 7th day (including weekends). After the 7th day, a grade of ‘0’ will be awarded even if the assessment is submitted. The submission time for all uploaded assessments is 11:55 pm. A 1-hour grace period will be provided to students who experience a technical concern. For any late submission of time-sensitive tasks, such as scheduled tests/exams, performance assessments/presentations, and/or scheduled practical assessments/labs, please apply for [Special Consideration](https://students.mq.edu.au/study/assessment-exams/special-consideration).\n",
    "\n",
    "Note that the work submitted should be your own work. You are allowed to use AI-based code generators to help you understand the problem and possible solutions, but you are not allowed to use the code generated by these tools (see below).\n",
    "\n",
    "You are allowed to base your code on the code presented in the unit lectures and lecture notebooks.\n",
    "\n",
    "**A note on the use of AI generators**: In this assignment, we view AI code generators such as copilot, CodeGPT, etc as tools that can help you write code quickly. You are allowed to use these tools, but with some conditions. To understand what you can and what you cannot do, please visit these information pages provided by Macquarie University.\n",
    "\n",
    "Artificial Intelligence Tools and Academic Integrity in FSE - https://bit.ly/3uxgQP4\n",
    "If you choose to use these tools, make the following explicit in your Jupyter notebook, under a section with heading \"Use of AI generators in this assignment\" :\n",
    "\n",
    "* What part of your code is based on the output of such tools,\n",
    "* What tools you used,\n",
    "* What prompts you used to generate the code or text, and\n",
    "* What modifications you made on the generated code or text.\n",
    "  \n",
    "This will help us assess your work fairly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Task Review\n",
    "\n",
    "In assignment 3 you will work on a task of \"query-focused summarisation\" on medical questions where the goal is, given a medical question and a list of sentences extracted from relevant medical publications, to determine which of these sentences from the list can be used as part of the answer to the question. Assignment 3 is divided into two parts. Part 1 will help you get familar with the data, and Part 2 requires you to implement deep neural networks.\n",
    "\n",
    "We will use data that has been derived from the **BioASQ challenge** (http://www.bioasq.org/), after some data manipulation to make it easier to process for this assignment. The BioASQ challenge organises several \"shared tasks\", including a task on biomedical semantic question answering which we are using here. The data are in the file `bioasq10_labelled.csv`, which is part of the zip file provided. Each row of the file has a question, a sentence text, and a label that indicates whether the sentence text is part of the answer to the question (1) or not (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Review\n",
    "\n",
    "The following code uses pandas to store the file `bioasq10_labelled.csv` in a data frame and show the first rows of data. For this code to run, first you need to unzip the file `data.zip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>question</th>\n",
       "      <th>sentence text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Hirschsprung disease (HSCR) is a multifactoria...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>In this study, we review the identification of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The majority of the identified genes are relat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>The non-Mendelian inheritance of sporadic non-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
       "      <td>Coding sequence mutations in e.g.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid  sentid                                           question  \\\n",
       "0    0       0  Is Hirschsprung disease a mendelian or a multi...   \n",
       "1    0       1  Is Hirschsprung disease a mendelian or a multi...   \n",
       "2    0       2  Is Hirschsprung disease a mendelian or a multi...   \n",
       "3    0       3  Is Hirschsprung disease a mendelian or a multi...   \n",
       "4    0       4  Is Hirschsprung disease a mendelian or a multi...   \n",
       "\n",
       "                                       sentence text  label  \n",
       "0  Hirschsprung disease (HSCR) is a multifactoria...      0  \n",
       "1  In this study, we review the identification of...      1  \n",
       "2  The majority of the identified genes are relat...      1  \n",
       "3  The non-Mendelian inheritance of sporadic non-...      1  \n",
       "4                  Coding sequence mutations in e.g.      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"bioasq10b_labelled.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the CSV file are:\n",
    "\n",
    "* `qid`: an ID for a question. Several rows may have the same question ID, as we can see above.\n",
    "* `sentid`: an ID for a sentence.\n",
    "* `question`: The text of the question. In the above example, the first rows all have the same question: \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"\n",
    "* `sentence text`: The text of the sentence.\n",
    "* `label`: 1 if the sentence is a part of the answer, 0 if the sentence is not part of the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Let's get started for the Part 2 tasks\n",
    "\n",
    "Use the provided files `training.csv`, `dev_test.csv`, and `test.csv` in the data.zip file for all the tasks below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VTTgRnN0dC4"
   },
   "source": [
    "# Task 1 (5 marks): Simple Siamese NN\n",
    "\n",
    "Implement a simple TensorFlow-Keras neural model that has the following sequence of layers:\n",
    "\n",
    "1. An input layer that will accept the tf.idf of triplet data. The input of Siamese network is a triplet, consisting of anchor (i.e., the question), positive answer, negative answer.\n",
    "2. 3 hidden layers and a relu activation function. You need to determine the size of the hidden layers.\n",
    "3. Implement a class that serves as a distance layer. It returns the squared Euclidean distance between anchor and positive answer, as well as that between anchor and negative answer\n",
    "4. Implement a function that prepares raw data in csv files into triplets. Note that it is important to keep the similar number of positive pairs and negative pairs. For example, if a question has 10 anwsers, then we at most can have 10 positive pairs and it is good to associate this question with 10~20 negative sentences. \n",
    "\n",
    "\n",
    "Train the model with the training data and use the `dev_test` set to determine a good size of the hidden layer. \n",
    "\n",
    "With the model that you have trained, implement a summariser that returns the $n$ sentences with highest predicted score. Use the following function signature:\n",
    "\n",
    "```{python}\n",
    "def nn_summariser(csvfile, questionids, n=1):\n",
    "   \"\"\"Return the IDs of the n sentences that have the highest predicted score. \n",
    "      The input questionids is a list of question ids. \n",
    "      The output is a list of lists of sentence ids\n",
    "   \"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "Report the final results using the test set. Remember: use the test set to report the final results of the best system only.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
    "* **1 mark** if the code passes input to the model correctly.\n",
    "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest prediction score in the given question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the hidden layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1. Prepare the Data\n",
    "\n",
    "  Implementing a simple Siamese Neural Network using TensorFlow-Keras.\n",
    " we'll implement a function to create triplets from the CSV data. This function will balance the number \n",
    " of positive and negative pairs for each question.\n",
    " Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def create_triplets(df):\n",
    "    triplets = []\n",
    "    grouped = df.groupby('qid')\n",
    "    \n",
    "    for qid, group in grouped:\n",
    "        positives = group[group['label'] == 1]\n",
    "        negatives = group[group['label'] == 0]\n",
    "        \n",
    "        for _, pos_row in positives.iterrows():\n",
    "            pos_sentence = pos_row['sentence text']\n",
    "            if len(negatives) > 0:\n",
    "                negative_samples = negatives.sample(n=min(len(negatives), 2))\n",
    "                for _, neg_row in negative_samples.iterrows():\n",
    "                    neg_sentence = neg_row['sentence text']\n",
    "                    triplets.append((pos_row['question'], pos_sentence, neg_sentence))\n",
    "    \n",
    "    return triplets\n",
    "\n",
    "# Load data\n",
    "training_df = pd.read_csv(\"training.csv\")\n",
    "dev_test_df = pd.read_csv(\"dev_test.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Create triplets\n",
    "train_triplets = create_triplets(training_df)\n",
    "dev_triplets = create_triplets(dev_test_df)\n",
    "test_triplets = create_triplets(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model:-  the Siamese Neural Network model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/_pywrap_cpu_feature_guard.so, 0x0002): Library not loaded: @rpath/libtensorflow_cc.2.dylib\n  Referenced from: <E232C875-F50C-32BA-8200-00E4EE27C31B> /Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n  Reason: tried: '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/../../_solib_darwin_arm64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so_Ucclib___Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so.runfiles/org_tensorflow/_solib_darwin_arm64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so_Ucclib___Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/../../_solib_darwin_arm64/_Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so.runfiles/org_tensorflow/_solib_darwin_arm64/_Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/../libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../../_solib_darwin_arm64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/_pywrap_cpu_feature_guard.so.runfiles/org_tensorflow/_solib_darwin_arm64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../../_solib_darwin_arm64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal_Umacos___Utensorflow_Spython/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/_pywrap_cpu_feature_guard.so.runfiles/org_tensorflow/_solib_darwin_arm64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal_Umacos___Utensorflow_Spython/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../../_solib_darwin_arm64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/_pywrap_cpu_feature_guard.so.runfiles/org_tensorflow/_solib_darwin_arm64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../../_solib_darwin_arm64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal_Umacos___Utensorflow_Spython/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/_pywrap_cpu_feature_guard.so.runfiles/org_tensorflow/_solib_darwin_arm64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal_Umacos___Utensorflow_Spython/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/bin/../lib/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/bin/../lib/libtensorflow_cc.2.dylib' (no such file), '/usr/local/lib/libtensorflow_cc.2.dylib' (no such file), '/usr/lib/libtensorflow_cc.2.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Dense, Lambda\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/__init__.py:45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[1;32m     43\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/core/ag_ctx.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[1;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/utils/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/utils/context_managers.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:45\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# pywrap_tensorflow must be imported first to avoid protobuf issues.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# (b/143110113)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# pylint: enable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/pywrap_tensorflow.py:34\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m self_check\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m self_check\u001b[38;5;241m.\u001b[39mpreload_check()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m   \u001b[38;5;66;03m# This import is expected to fail if there is an explicit shared object\u001b[39;00m\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;66;03m# dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/self_check.py:63\u001b[0m, in \u001b[0;36mpreload_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find the DLL(s) \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m. TensorFlow requires that these DLLs \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe installed in a directory that is named in your \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m           \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;66;03m# Load a library that performs CPU feature guard checking.  Doing this here\u001b[39;00m\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;66;03m# as a preload check makes it more likely that we detect any CPU feature\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;66;03m# incompatibilities before we trigger them (which would typically result in\u001b[39;00m\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;66;03m# SIGILL).\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_cpu_feature_guard\n\u001b[1;32m     64\u001b[0m   _pywrap_cpu_feature_guard\u001b[38;5;241m.\u001b[39mInfoAboutUnusedCPUFeatures()\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/_pywrap_cpu_feature_guard.so, 0x0002): Library not loaded: @rpath/libtensorflow_cc.2.dylib\n  Referenced from: <E232C875-F50C-32BA-8200-00E4EE27C31B> /Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n  Reason: tried: '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/../../_solib_darwin_arm64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so_Ucclib___Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so.runfiles/org_tensorflow/_solib_darwin_arm64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so_Ucclib___Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/../../_solib_darwin_arm64/_Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so.runfiles/org_tensorflow/_solib_darwin_arm64/_Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/../libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../../_solib_darwin_arm64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/_pywrap_cpu_feature_guard.so.runfiles/org_tensorflow/_solib_darwin_arm64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../../_solib_darwin_arm64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal_Umacos___Utensorflow_Spython/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/_pywrap_cpu_feature_guard.so.runfiles/org_tensorflow/_solib_darwin_arm64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal_Umacos___Utensorflow_Spython/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../../_solib_darwin_arm64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/_pywrap_cpu_feature_guard.so.runfiles/org_tensorflow/_solib_darwin_arm64/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../../_solib_darwin_arm64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal_Umacos___Utensorflow_Spython/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/_pywrap_cpu_feature_guard.so.runfiles/org_tensorflow/_solib_darwin_arm64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal_Umacos___Utensorflow_Spython/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/bin/../lib/libtensorflow_cc.2.dylib' (no such file), '/Users/rakshaghutke/anaconda3/bin/../lib/libtensorflow_cc.2.dylib' (no such file), '/usr/local/lib/libtensorflow_cc.2.dylib' (no such file), '/usr/lib/libtensorflow_cc.2.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_siamese_model(input_shape, hidden_size):\n",
    "    input_anchor = Input(shape=(input_shape,))\n",
    "    input_positive = Input(shape=(input_shape,))\n",
    "    input_negative = Input(shape=(input_shape,))\n",
    "    \n",
    "    # Shared layers\n",
    "    shared_dense_1 = Dense(hidden_size, activation='relu')\n",
    "    shared_dense_2 = Dense(hidden_size, activation='relu')\n",
    "    shared_dense_3 = Dense(hidden_size, activation='relu')\n",
    "    \n",
    "    # Anchor network\n",
    "    anchor = shared_dense_1(input_anchor)\n",
    "    anchor = shared_dense_2(anchor)\n",
    "    anchor = shared_dense_3(anchor)\n",
    "    \n",
    "    # Positive network\n",
    "    positive = shared_dense_1(input_positive)\n",
    "    positive = shared_dense_2(positive)\n",
    "    positive = shared_dense_3(positive)\n",
    "    \n",
    "    # Negative network\n",
    "    negative = shared_dense_1(input_negative)\n",
    "    negative = shared_dense_2(negative)\n",
    "    negative = shared_dense_3(negative)\n",
    "\n",
    " # Distance layer\n",
    "    def euclidean_distance(vects):\n",
    "        x, y = vects\n",
    "        return tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True)\n",
    "    \n",
    "    positive_distance = Lambda(euclidean_distance)([anchor, positive])\n",
    "    negative_distance = Lambda(euclidean_distance)([anchor, negative])\n",
    "    \n",
    "    model = Model(inputs=[input_anchor, input_positive, input_negative], outputs=[positive_distance, negative_distance])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define input shape and hidden layer size\n",
    "input_shape = 10000  # Assuming 10,000 features from TF-IDF\n",
    "hidden_size = 256\n",
    "\n",
    "siamese_model = create_siamese_model(input_shape, hidden_size)\n",
    "siamese_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uY6sDbUn0dC6"
   },
   "outputs": [],
   "source": [
    "# Write your code and answers here. Feel free to add more code and markdown cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0NeK3gM0dC9"
   },
   "source": [
    "# Task 2 (5 marks): Recurrent NN\n",
    "\n",
    "Implement a more complex Siamese neural network that is composed of the following layers:\n",
    "\n",
    "* An embedding layer that generates embedding vectors of the sentence text with 35 dimensions.\n",
    "* A LSTM layer. You need to determine the size of this LSTM layer, and the text length limit (if needed).\n",
    "* 3 hidden layers and a relu activation function. You need to determine the size of the hidden layers.\n",
    "\n",
    "Train the model with the training data, use the `dev_test` set to determine a good size of the LSTM layer and an appropriate length limit (if needed), and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the LSTM layer.\n",
    "\n",
    "Based on your experiments, comment on whether this system is better than the systems developed in the previous tasks.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
    "* **1 mark** if the code passes the sentence text to the model correctly. The documentation needs to explain what decisions had to be made to process long sentences. In particular, did you need to truncate the input text, and how did you determine the length limit?\n",
    "* **1 mark** if the code returns the IDs of the *n* sentences that have the highest prediction score in the given question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the LSTM layer (and length limit) and hidden layers. The explanations must be clear and concise. To make this task less time-consuming, use $n=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c8RRCWeQTrPl"
   },
   "outputs": [],
   "source": [
    "# Write your code and answers here. Feel free to add more code and markdown cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (5 marks): Transformer\n",
    "\n",
    "Implement a simple Transformer neural network that is composed of the following layers:\n",
    "\n",
    "* Use BERT as feature extractor for each token.\n",
    "* A few of transformer encoder layers, hidden dimension 768. You need to determine how many layers to use between 1~3.\n",
    "* A few of transformer decoder layers, hidden dimension 768. You need to determine how many layers to use between 1~3.\n",
    "* 1 hidden layer with size 512.\n",
    "* The final output layer with one cell for binary classification to predict whether two inputs are related or not. \n",
    "\n",
    "Note that each input for this model should be a concatenation of a positive pair (i.e. question + one answer) or a negative pair (i.e. question + not related sentence). The format is usually like [CLS]+ question + [SEP] + a positive/negative sentence.\n",
    "\n",
    "Train the model with the training data, use the dev_test set to determine a good size of the transformer layers, and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the transformer layers.\n",
    "\n",
    "Based on your experiments, comment on whether this system is better than the systems developed in the previous tasks.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the model has the correct layers, the correct activation functions, and the correct loss function.\n",
    "* **1 mark** if the code passes the sentence text to the model correctly. The documentation needs to explain how to handle length difference for a batch of data\n",
    "* **1 mark** if the code returns the IDs of the *n* sentences that have the highest prediction score in the given question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the transformer layers. The explanations must be clear and concise. To make this task less time-consuming, use $n=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppkBsuB_0dC9"
   },
   "source": [
    "# Submission \n",
    "\n",
    "Your submission should consist of this Jupyter notebook with all your code and explanations inserted into the notebook as text cells. **The notebook should contain the output of the runs. All code should run. Code with syntax errors or code without output will not be assessed.**\n",
    "\n",
    "**Do not submit multiple files.**\n",
    "\n",
    "Examine the text cells of this notebook so that you can have an idea of how to format text for good visual impact. You can also read this useful [guide to the MarkDown notation](https://daringfireball.net/projects/markdown/syntax),  which explains the format of the text cells."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "A2_solution.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
